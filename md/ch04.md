# 4章word2vecの高速化

## 4.1 word2vecの改良①
### 4.1.1 Embeddingレイヤ
### 4.1.2 Embeddingレイヤの実装

～  

```julia
julia> W = reshape(collect(0:20), 7,3)
7×3 Matrix{Int64}:
 0   7  14
 1   8  15
 2   9  16
 3  10  17
 4  11  18
 5  12  19
 6  13  20

julia> selectdim(W, 1, 3)
3-element Vector{Int64}:
  2
  9
 16

julia> selectdim(W, 1, 6)
3-element Vector{Int64}:
  5
 12
 19

```

～  

```julia
julia> idx = [2, 1, 4, 1];

julia> selectdim(W, 1, idx)
4×3 Matrix{Int64}:
 1   8  15
 0   7  14
 3  10  17
 0   7  14
```

～  

```julia
mutable struct Embedding
    params
    grads
    idx
end

Embedding(W) = Embedding([W], [zero(W)], nothing)

function forward(self::Embedding, idx)
    W = self.params[1]
    self.idx = idx
    out = selectdim(W, 1, idx)
    return out
end
```

～  

```julia
function backward(self::Embedding, dout)
    dW = self.grads[1]
    dW .= 0.0
    
    selectdim(dW, 1, self.idx) .= dout # 実は悪い例
    return nothing
end
```

～  

```julia
function backward(self::Embedding, dout)
    dW = self.grads[1]
    dW .= 0.0
    
    selectdim(dW, 1, self.idx) .+= dout
    return nothing
end
```

～  

## 4.2 word2vecの改良②
### 4.2.1 中間層以降の計算の問題点
### 4.2.2 多値分類から二値分類へ
### 4.2.3 シグモイド関数と交差エントロピー誤差
### 4.2.4 多値分類から二値分類へ（実装編）

～  

```julia
mutable struct EmbeddingDot
    embed
    params
    grads
    cache
end
function EmbeddingDot(W)
    embed = Embedding(W)
    params = embed.params
    grads = embed.grads
    return EmbeddingDot(embed, params, grads, nothing)
end

function forward(self::EmbeddingDot, h, idx)
    target_W = forward(self.embed, idx)
    out = sum(target_W * h, dims=2)

    self.cache = (h, target_W)
    return out
end

function backward(self::EmbeddingDot, dout)
    h, target_W = self.cache
    dout = reshape(dout, size(dout, 1), 1)

    dtarget_W = dout * h
    backward(self.embed, dtarget_W)
    dh = dout .* target_W
    return dh
end
```

～  

### 4.2.5 Negative Sampling
### 4.2.6 Negative Samplingのサンプリング手法

～  

```julia
julia> import Random: shuffle

# 0から9の数字の中からひとつの数字をランダムにサンプリング
julia> rand(0:9)
2

julia> rand(0:9)
8

# wordsからひとつだけランダムにサンプリング
julia> words = ["you", "say", "goodbye", "I", "hello", "."];

julia> rand(words)
"you"

# 5つだけランダムサンプリング（重複あり）
julia> rand(words, 5)
5-element Vector{String}:
 "I"
 "hello"
 "I"
 "hello"
 "you"

# 5つだけランダムサンプリング（重複なし）
julia> shuffle(words)[1:5]
5-element Vector{String}:
 "hello"
 "you"
 "say"
 "I"
 "goodbye"

julia> function choice(array; p)
           array = collect(array)
           l = min(length(array), length(p))
           r = rand()
           p = [sum(p[1:i]) for i=1:l]/sum(p[1:l])
           for i=1:l
               if p[i]>=r
                   return array[i]
               end
           end
       end

# 確率分布に従ってサンプリング
julia> choice(words, p=[0.5, 0.1, 0.05, 0.2, 0.05, 0.1])
"you"
```

～  

```julia
julia> p = [0.7 0.29 0.01]
1×3 Matrix{Float64}:
 0.7  0.29  0.01

julia> new_p = p.^0.75
1×3 Matrix{Float64}:
 0.765286  0.395183  0.0316228

julia> new_p /= sum(new_p)
1×3 Matrix{Float64}:
 0.641969  0.331504  0.0265271
```

～  

```julia
corpus = [1,2,3,4,5,2,3,4]
power = 0.75
sample_size = 2

sampler = UnigramSampler(corpus, power, sample_size)
target = [2, 4, 1]
negative_sample = get_negative_sample(sampler, target)
# 3×2 Matrix{Int32}:
#  4  5
#  3  2
#  4  3
```

～  

### 4.2.7 Negative Samplingの実装

～  

```julia
mutable struct NegativeSamplingLoss
    sample_size
    sampler
    loss_layers
    embed_dot_layers
    params
    grads
end

function NegativeSamplingLoss(W, corpus; power=0.75, sample_size=5)
    sample_size = sample_size
    sampler = UnigramSampler(corpus, power, sample_size)
    loss_layers = [SigmoidWithLoss() for _ = 0:sample_size]
    embed_dot_layers = [EmbeddingDot(W) for _ = 0:sample_size]

    self.params, self.grads = [], []
    params = typeof(embed_dot_layers[1].params)([])
    grads  = typeof(embed_dot_layers[1].grads)([])
    for layer = embed_dot_layers
        append!(params, layer.params)
        append!(grads,  layer.grads)
    end

    return NegativeSamplingLoss(sample_size, sampler, loss_layers, embed_dot_layers, params, grads)
end
```

～  

```julia
function forward(self::NegativeSamplingLoss, h, target)
    batch_size = size(target, 1)
    negative_sample = get_negative_sample(self.sampler, target)

    # 正例のフォワード
    score = forward(self.embed_dot_layers[1], h, target)
    correct_label = ones(Int32, batch_size)
    loss = forward(self.loss_layers[1], score, correct_label)

    # 負例のフォワード
    negative_label = zeros(Int32, batch_size)
    for i = 1:self.sample_size
        negative_target = selectdim(negative_sample[:, i], 2, i)
        score = forward(self.embed_dot_layers[1 + i], h, negative_target)
        loss += forward(self.loss_layers[1 + i], score, negative_label)
    end
    return loss
end
```

～  

```julia
function backward(self::NegativeSamplingLoss, dout=1)
    dh = 0
    for (l0, l1) = zip(self.loss_layers, self.embed_dot_layers)
        dscore = backward(l0, dout)
        dh += backward(l1, dscore)
    end
    return dh
end
```

～  

## 4.3 改良版word2vecの学習
### 4.3.1 CBOWモデルの実装

～  

```julia
mutable struct CBOW
    in_layers
    ns_loss
    params
    grads
    word_vecs
end

function CBOW(vocab_size, hidden_size, window_size, corpus)
    V, H = vocab_size, hidden_size

    # 重みの初期化
    W_in = 0.01 * randn(Float32, V, H)
    W_out = 0.01 * randn(Float32, V, H)

    # レイヤの生成
    in_layers = [Embedding(W_in) for i=1:(2*window_size)] # Embeddingレイヤを使用

    ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)

    # すべての重みと勾配をリストにまとめる
    layers = [in_layers..., ns_loss]
    params = typeof(layers[1].params)([])
    grads  = typeof(layers[1].grads)([])
    for layer in layers
        append!(params, layer.params)
        append!(grads,  layer.grads)
    end

    # メンバ変数に単語の分散表現を設定
    word_vecs = W_in

    return CBOW(in_layers, ns_loss, params, grads, word_vecs)
end
```

～  

```julia
function forward(self::CBOW, contexts, target)
    h = 0
    for (i, layer) = enumerate(self.in_layers)
        h += forward(layer, selectdim(contexts, 2, i))
    end
    h /= length(self.in_layers)
    loss = forward(self.ns_loss, h, target)
    return loss
end

function backward(self::CBOW, dout=1)
    dout = backward(self.ns_loss, dout)
    dout /= length(self.in_layers)
    for layer = self.in_layers
        backward(layer, dout)
    end
end
```

～  

### 4.3.2 CBOWモデルの学習コード

～  

```julia
import JLD2
include("../common/trainer.jl") # Trainer
include("../common/optimizer.jl") # Adam
include("cbow.jl") # CBOW
include("skip_gram.jl") # SkipGram
include("../common/util.jl") # create_contexts_target, to_cpu, to_gpu
include("../dataset/ptb.jl")


# ハイパーパラメータの設定
window_size = 5
hidden_size = 100
batch_size = 100
max_epoch = 10

# データの読み込み
corpus, word_to_id, id_to_word = load_data("train")
vocab_size = length(word_to_id)

contexts, target = create_contexts_target(corpus, window_size=window_size)

# モデルなどの生成
model = CBOW(vocab_size, hidden_size, window_size, corpus)
optimizer = Adam()
trainer = Trainer(model, optimizer)

# 学習開始
trainer.fit(contexts, target, max_epoch, batch_size)
trainer.plot()

# 後ほど利用できるように、必要なデータを保存
word_vecs = model.word_vecs
params = Dict(
    "word_vecs" => Float16.(word_vecs),
    "word_to_id" => word_to_id,
    "id_to_word" => id_to_word
)
jld2_file = "cbow_params.jld2"
JLD2.save(jld2_file, params)
```

～  

### 4.3.3 CBOWモデルの評価


## 4.4 word2vecに関する残りのテーマ
### 4.4.1 word2vecを使ったアプリケーションの例
### 4.4.2 単語ベクトルの評価方法


## 4.5 まとめ 
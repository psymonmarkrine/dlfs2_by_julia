# 3章 word2vec

## 3.1 推論ベースの手法とニューラルネットワーク
### 3.1.1 カウントベースの手法の問題点
### 3.1.2 推論ベースの手法の概要
### 3.1.3 ニューラルネットワークにおける単語の処理方法

～  

```julia
c = [1 0 0 0 0 0 0]
W = randn(7, 3)
h = c * W
println(h)
# [0.7762100406541127 -0.04444834933302676 -0.03835470527346903]
```

～  

```julia
include("../common/layers.jl") # MatMul

c = [1 0 0 0 0 0 0]
W = randn(7, 3)
layer = MatMul(W)
h = forward(layer, c)
println(h)
# [1.5928744352708335 0.300612256900718 1.4042507868919796]
```

～  

```julia
include("../common/layers.jl") # MatMul


# サンプルのコンテキストデータ
c0 = [1 0 0 0 0 0 0]
c1 = [0 0 1 0 0 0 0]

# 重みの初期化
W_in = randn(7, 3)
W_out = randn(3, 7)

# レイヤの生成
in_layer0 = MatMul(W_in)
in_layer1 = MatMul(W_in)
out_layer = MatMul(W_out)

# 順伝搬
h0 = forward(in_layer0, c0)
h1 = forward(in_layer1, c1)
h = 0.5 * (h0 + h1)
s = forward(out_layer, h)
println(s)
# [1.766848633729323 2.524990511096121 0.20693052476884358 -0.061341396755521936 -1.0306668199248308 0.13008490778960297 -0.11827894343931313]
```

## 3.2 シンプルなword2vec
### 3.2.1 CBOWモデルの推論処理
### 3.2.2 CBOWモデルの学習
### 3.2.3 word2vecの重みと分散表現
## 3.3 学習データの準備
### 3.3.1 コンテキストとターゲット

～  

```julia
include("../common/util.jl") # preprocess

text = "You say goodbye and I say hello."
corpus, word_to_id, id_to_word = preprocess(text)
println(corpus)
# [1, 2, 3, 4, 5, 2, 6, 7]

println(id_to_word)
# Dict{Integer, AbstractString}(5 => "i", 4 => "and", 6 => "hello", 7 => ".", 2 => "say", 3 => "goodbye", 1 => "you")
```

～  

```julia
function create_contexts_target(corpus; window_size=1)
    """コンテキストとターゲットの作成
    :param corpus: コーパス（単語IDのリスト）
    :param window_size: ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）
    :return:
    """
    target = corpus[(window_size+1):(end-window_size)]
    contexts = zeros(eltype(corpus), (0, 2window_size))

    for idx = (window_size+1):(length(corpus)-window_size)
        cs = zeros(eltype(corpus), (1, 0))
        for t = -window_size:window_size
            if t == 0
                continue
            end
            cs = hcat(cs, corpus[idx + t])
        end
        contexts = vcat(contexts, cs)
    end
    return contexts, target
end
```

～  

```julia
contexts, target = create_contexts_target(corpus, 1)

display(contexts)
# 6×2 Matrix{Int64}:
#  1  3
#  2  4
#  3  5
#  4  2
#  5  6
#  2  7

println(target)
# [2, 3, 4, 5, 2, 6]
```

### 3.3.2 one-hot表現への変換

～  

```julia
include("../common/util.jl") # preprocess, create_contexts_target, convert_one_hot

text = "You say goodbye and I say hello."
corpus, word_to_id, id_to_word = preprocess(text)

contexts, target = create_contexts_target(corpus, window_size=1)

vocab_size = length(word_to_id)
target = convert_one_hot(target, vocab_size)
contexts = convert_one_hot(contexts, vocab_size)
```

～  

## 3.4 CBOWモデルの実装

～  

```julia
include("../common/layers.jl") # MatMul, SoftmaxWithLoss


mutable struct SimpleCBOW
    in_layer0
    in_layer1
    out_layer
    loss_layer
    layers
    params
    grads
    word_vecs
end

function SimpleCBOW(vocab_size::Integer, hidden_size::Integer)
    V, H = vocab_size, hidden_size

    # 重みの初期化
    W_in = 0.01f0 * randn(Float32, (V, H))
    W_out = 0.01f0 * randn(Float32, (H, V))

    # レイヤの生成
    in_layer0 = MatMul(W_in)
    in_layer1 = MatMul(W_in)
    out_layer = MatMul(W_out)
    loss_layer = SoftmaxWithLoss()

    # すべての重みと勾配をリストにまとめる
    layers = [in_layer0, in_layer1, out_layer]
    params = [layer.params for layer = layers]
    grads  = [layer.grads  for layer = layers]
    
    # メンバ変数に単語の分散表現を設定
    word_vecs = W_in
    return SimpleCBOW(in_layer0, in_layer1, out_layer, loss_layer, layers, params, grads, word_vecs)
end
```

～  

```julia
function forward(self::SimpleCBOW, contexts, target)
    h0 = forward(self.in_layer0, contexts[:, 0])
    h1 = forward(self.in_layer1, contexts[:, 1])
    h = (h0 + h1) * 0.5
    score = forward(self.out_layer, h)
    loss  = forward(self.loss_layer, score, target)
    return loss
end
```

～  

```julia
function backward(self::SimpleCBOW, dout=1)
    ds = backward(self.loss_layer, dout)
    da = backward(self.out_layer, ds)
    da *= 0.5
    backward(self.in_layer1, da)
    backward(self.in_layer0, da)
end
```

～  

### 3.4.1 学習コードの実装

～  

```julia
include("../common/trainer.jl") # Trainer
include("../common/optimizer.jl") # Adam
include("simple_cbow.jl") # SimpleCBOW
include("../common/util.jl") # preprocess, create_contexts_target, convert_one_hot


window_size = 1
hidden_size = 5
batch_size = 3
max_epoch = 1000

text = "You say goodbye and I say hello."
corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = length(word_to_id)
contexts, target = create_contexts_target(corpus, window_size=window_size)
target = convert_one_hot(target, vocab_size)
contexts = convert_one_hot(contexts, vocab_size)

model = SimpleCBOW(vocab_size, hidden_size)
optimizer = Adam()
trainer = Trainer(model, optimizer)

fit(trainer, contexts, target, max_epoch, batch_size)
plot(trainer)
```

～  

```julia
word_vecs = model.word_vecs
for (word_id, word) = id_to_word
    println("$word $(word_vecs[word_id,:])")
end
```

～  

```julia
i Float32[-0.007260677, -0.0108879525, -0.013124989, -0.014340146, -0.008050979]
and Float32[0.004087626, 0.0063180844, -0.0030210048, -0.014079179, 0.0058099013]
hello Float32[-0.00079573935, -0.016594404, -0.005469534, 0.005995057, 0.029301884]
. Float32[-0.0074867033, 0.0029805994, 0.009366218, 0.004222983, 0.006562579]
say Float32[0.00065140834, -0.003510901, -0.0103187235, -0.016411303, 0.0006512841]
goodbye Float32[-0.005116437, 0.006946308, -0.004873838, -0.0016211099, -0.005146629]
you Float32[-0.0041065523, 0.010234824, 0.014737297, -0.009469126, -0.008621897]
```

～  

## 3.5 word2vecに関する補足
### 3.5.1 CBOWモデルと確率
### 3.5.2 skip-gramモデル
### 3.5.3 カウントベース v.s. 推論ベース
## 3.6 まとめ
# 5章　リカレントニューラルネットワーク（RNN）

## 5.1 確率と言語モデル
### 5.1.1 word2vecを確率の視点から眺める
### 5.1.2 言語モデル
### 5.1.3 CBOWモデルを言語モデルに？

## 5.2 RNNとは
### 5.2.1 循環するニューラルネットワーク
### 5.2.2 ループの展開
### 5.2.3 Backpropagation Through Time
### 5.2.4 Truncated BPTT
### 5.2.5 Truncated BPTTのミニバッチ学習

## 5.3 RNNの実装
### 5.3.1 RNNレイヤの実装

～  

```julia
mutable struct RNN
    params
    grads
    cache
end

function RNN(Wx, Wh, b::Vector)
    params = [Wx, Wh, b]
    grads = [zero(Wx), zero(Wh), zero(b)]
    cache = nothing
    return RNN(params, grads, cache)
end

function forward(self::RNN, x, h_prev)
    Wx, Wh, b = self.params
    t = h_prev * Wh + x * Wx + b
    h_next = tanh(t)

    self.cache = (x, h_prev, h_next)
    return h_next
end
```

～  

```julia
function backward(self::RNN, dh_next)
    Wx, Wh, b = self.params
    x, h_prev, h_next = self.cache

    dt = @. dh_next * (1 - h_next ^ 2)
    db = sum(dt, dims=1)
    dWh = h_prev' * dt
    dh_prev = dt * Wh'
    dWx = x' * dt
    dx = dt * Wx.'

    self.grads[0] .= dWx
    self.grads[1] .= dWh
    self.grads[2] .= db

    return dx, dh_prev
end
```

～  

### 5.3.2 Time RNNレイヤの実装

## 5.4 時系列データを扱うレイヤの実装
### 5.4.1 RNNLMの全体図
### 5.4.2 Timeレイヤの実装

## 5.5 RNNLMの学習と評価
### 5.5.1 RNNLMの実装
### 5.5.2 言語モデルの評価
### 5.5.3 RNNLMの学習コード
### 5.5.4 RNNLMのTrainerクラス

## 5.6 まとめ
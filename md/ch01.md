# 1章 ニューラルネットワークの復習

本書は、前作『ゼロから作る Deep Learning』の続編です。  
～  

## 1.1 数学とPythonの復習

まず初めに数学の復習から始めます。  
具体的には、ニューラルネットワークの計算に必要な「ベクトル」や「行列」などをテーマに話を進めていきます。  
また、ニューラルネットワークの実装にスムーズに入っていけるように、Juliaによるコードも併せて示していきます。  

### 1.1.1 ベクトルと行列

～  
それでは、Juliaの対話モードを使って、ベクトルや行列を生成してみましょう。  

```julia
julia> x = [1, 2, 3]
3-element Vector{Int64}:
 1
 2
 3

julia> typeof(x)
Vector{Int64} (alias for Array{Int64, 1})

julia> size(x)
(3,)

julia> ndims(x)
1

julia> W = [1 2 3; 4 5 6]
2×3 Matrix{Int64}:
 1  2  3
 4  5  6

julia> size(W)
(2, 3)

julia> ndims(W)
2

```

### 1.1.2 行列の要素ごとの演算

～  
ここでは初めに「要素ごとの演算」についてみていきます。  

```julia
julia> W = [1 2 3; 4 5 6]
2×3 Matrix{Int64}:
 1  2  3
 4  5  6

julia> X = [0 1 2; 3 4 5]
2×3 Matrix{Int64}:
 0  1  2
 3  4  5

julia> W+X
2×3 Matrix{Int64}:
 1  3   5
 7  9  11

julia> W .* X
2×3 Matrix{Int64}:
  0   2   6
 12  20  30
```

ここでは多次元配列に対して、四則演算を行っています。  
Juliaでは、行列に対する演算は数学的な行列演算に従います。  
そのため、行列同士の`*`演算は行列積として扱われてしまうため、`.`を付けて`W .* X`とすることで要素ごとに積を求めることができます。  
行列同士の和は要素ごとに和を求めるため、`W + X`と`W .+ X`どちらも同様の結果を得られます。  

### 1.1.3 ブロードキャスト

形状の異なる配列どうしの演算も可能です。  
たとえば次のような演算です。  

```julia
julia> A = [1 2; 3 4]
2×2 Matrix{Int64}:
 1  2
 3  4

julia> b = [10 20]
1×2 Matrix{Int64}:
 10  20

julia> A .* b
2×2 Matrix{Int64}:
 10  40
 30  80
```

この計算では、1次元配列であるbが2次元行列Aと同じ形状になるように“賢く”拡張されます。  
この賢い機能は、`ブロードキャスト`（broadcast）と呼ばれます。  

|# Tips|
|:-|
|ブロードキャストが有効に働くには、多次元配列の形状がいくつかのルールを満たす必要があります。|
|Juliaのブロードキャストについては[このページ](https://docs.julialang.org/en/v1/manual/arrays/#Broadcasting)などを参照してください。|

### 1.1.4 ベクトルの内積と行列の積

～  
それでは、ベクトルの内積と行列の積をJuliaで実装してみましょう。  

```julia
julia> a = [1, 2, 3]
3-element Vector{Int64}:
 1
 2
 3

julia> b = [4, 5, 6]
3-element Vector{Int64}:
 4
 5
 6

julia> a' * b # ベクトルの内積
32

julia> A = [1 2; 3 4]
2×2 Matrix{Int64}:
 1  2
 3  4

julia> B = [5 6; 7 8]
2×2 Matrix{Int64}:
 5  6
 7  8

julia> A * B # 行列の積
2×2 Matrix{Int64}:
 19  22
 43  50

```

ここで示したように、配列の積は行列積として計算されます。  
ベクトルは1列の行列として扱われるため「`'`」を付けて転置して行列積を求めることでベクトルの内積を求めることができます。  

### 1.1.5 行列の形状チェック
## 1.2 ニューラルネットワークの推論
### 1.2.1 ニューラルネットワークの推論の全体図

～  

```julia
W1 = randn(2, 4) # 重み
b1 = randn(1, 4) # バイアス
x = randn(10, 2) # 入力
h = x * W1 .+ b1
```

～  

```julia
function sigmoid(x)
    return 1 / (1 + exp(-x))
end
```

～  

```julia
function sigmoid(x)
    return 1 / (1 + exp(-x))
end

x = randn(10, 2)
W1 = randn(2, 4)
b1 = randn(1, 4)
W2 = randn(4, 3)
b2 = randn(1, 3)

h = x * W1 .+ b1
a = sigmoid.(h)
s = a * W2 .+ b2
```

～  

### 1.2.2 レイヤとしてのクラス化と順伝播の実装

～  

```julia
mutable struct Sigmoid
    params
end

Sigmoid() = Sigmoid([])

function forward(self::Sigmoid, x)
    @. return 1 / (1 + exp(-x))
end
```

～  

```julia
mutable struct Affine
    params
end

Affine(W, b) = Affine([W, b])

function forward(self::Affine, x)
    W, b = seld.params
    out = x * W .+ b
    return out
end
```

～  

```julia
mutable struct TwoLayerNet
    params
    layers
end

function TwoLayerNet(input_size, hidden_size, output_size)
    I, H, O = input_size, hidden_size, output_size

    # 重みとバイアスの初期化
    W1 = randn(I, H)
    b1 = randn(1, H)
    W2 = randn(H, O)
    b2 = randn(1, O)

    # レイヤの生成
    layers = [
        Affine(W1, b1),
        Sigmoid(),
        Affine(W2, b2)
    ]

    # すべての重みをリストにまとめる
    params = []
    for layer = layers
        append!(params, layer.params)
    end

    return TwoLayerNet(params, layers)
end

function predict(self::TwoLayerNet, x)
    for layer = self.layers
        x = forward(layer, x)
    end
    return x
end
```

～  

```julia
x = randn(10, 2)
model = TwoLayerNet(2, 4, 3)
s = predict(model, x)
println(s)
```

～  

## 1.3 ニューラルネットワークの学習
### 1.3.1 損失関数
### 1.3.2 微分と勾配
### 1.3.3 チェインルール
### 1.3.4 計算グラフ

#### 1.3.4.3 Repeatノード

～  

```julia
julia> D, N = 8, 7
(8, 7)

julia> x = randn(1, D);  # 入力

julia> y = repeat(x, N); # forward

julia> dy = randn(N, D); # 仮の勾配

dx = sum(dy, dims=1);    # backward
```

～  

#### 1.3.4.4 Sumノード

～  

```julia
julia> D, N = 8, 7
(8, 7)

julia> x = randn(N, D);    # 入力

julia> y = sum(x, dims=1); # forward

julia> dy = randn(1, D);   # 仮の勾配

julia> dx = repeat(dy, N); # backward
```

～  

#### 1.3.4.5 MatMulノード

～  

```julia
mutable struct MatMul
    params
    grads
    x
end

MatMul(W) = MatMul([W], [zero(W)], nothing)

function forward(self::MatMul, x)
    W = self.params[1]
    out = x * W
    self.x = x
    return out
end
function backward(self::MatMul, dout)
    W = self.params[1]
    dx = dout * W'
    dW = self.x' * dout
    self.grads[1] .= dW
    return dx
end
```

～  
なお、勾配の値を設定する際に、`grads[1] .= dW`というように「`.=`」を使っています。  
この「`.=`」を用いることで、配列のメモリ位置を固定したうえで、配列の要素を上書きします。  

|Tips|
|:-|
|「`.=`」と同じようなことは、`grads[1] = dW`の「代入」によっても行えます。|
|一方、「`.=`」の場合は、配列の「上書き」が行われます。|
|これは「浅いコピー（shallow copy）」か「深いコピー（deep copy）」かという違いです。|
|`grads[1] = dW`の代入は「浅いコピー」、`grads[1] .= dW`の上書きは「深いコピー」に相当します。|

～  

### 1.3.5 勾配の導出と逆伝播の実装
#### 1.3.5.1 Sigmoidレイヤ

～  

```julia
mutable struct Sigmoid
    params
    grads
    out
end

Sigmoid() = Sigmoid([], [], nothing)

function forward(self::Sigmoid, x)
    out = @. 1 / (1 + exp(-x))
    self.out = out
    return out
end

function backward(self::Sigmod, dout)
    dx = @. dout * (1.0 - self.out) * self.out
    return dx
end
```

～  

#### 1.3.5.2 Affineレイヤ

～  

```julia
mutable struct Affine
    params
    grads
    x
end

Affine(W, b) = Affine([W, b], [zero(W), zero(b)], nothing)

function forward(self::Affine, x)
    W, b = self.params
    out = x * W .+ b
    self.x = x
    return out
end

function backward(self::Affine, dout)
    W, b = self.params
    dx = dout * W"
    dW = self.x" * dout
    db = sum(dout, dims=1)

    self.grads[1] .= dW
    self.grads[2] .= db
    return dx
end
```

### 1.3.6 重みの更新

～  

```julia
mutable struct SGD
    lr
    SGD(lr=0.01) = new(lr)
end

function update(self::SGD, params, grads)
    for i = 1:length(params)
        params[i] .-= self.lr * grads[i]
    end
end
```

～  

```julia
model = TwoLayerNet(...)
optimizer = SGD()

for i=1:10000
    ...
    x_batch, t_batch = get_mini_batch(...) # ミニバッチの取得
    loss = forward(model, x_batch, t_batch)
    backward(model)
    update(optimizer, model.params, model.grads)
    ...

end
```

～  

## 1.4 ニューラルネットワークで問題を解く
### 1.4.1 スパイラル・データセット
### 1.4.2 ニューラルネットワークの実装
### 1.4.3 学習用のソースコード
### 1.4.4 Trainerクラス

## 1.5 計算の高速化
### 1.5.1 ビット精度
### 1.5.2 GPU（CuPy）

## 1.6 まとめ

～  
